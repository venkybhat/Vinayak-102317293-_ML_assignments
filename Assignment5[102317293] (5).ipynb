{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1)\n",
        "(Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
        "Descent Optimization)\n",
        "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
        "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
        "parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum."
      ],
      "metadata": {
        "id": "6SLYUxqL_yEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_98V1iaz8z0k",
        "outputId": "c6db24cf-469e-46e4-e270-0ba014ba74cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr=0.0001, lam=1e-15, cost=775.02358, r2=-1.48343\n",
            "lr=0.0001, lam=1e-10, cost=775.02358, r2=-1.48343\n",
            "lr=0.0001, lam=1e-05, cost=775.02358, r2=-1.48343\n",
            "lr=0.0001, lam=0.001, cost=775.02367, r2=-1.48343\n",
            "lr=0.0001, lam=0, cost=775.02358, r2=-1.48343\n",
            "lr=0.0001, lam=1, cost=775.1138, r2=-1.48354\n",
            "lr=0.0001, lam=10, cost=775.92333, r2=-1.48453\n",
            "lr=0.0001, lam=20, cost=776.81786, r2=-1.48562\n",
            "lr=0.001, lam=1e-15, cost=117.33338, r2=0.62403\n",
            "lr=0.001, lam=1e-10, cost=117.33338, r2=0.62403\n",
            "lr=0.001, lam=1e-05, cost=117.33338, r2=0.62403\n",
            "lr=0.001, lam=0.001, cost=117.3336, r2=0.62403\n",
            "lr=0.001, lam=0, cost=117.33338, r2=0.62403\n",
            "lr=0.001, lam=1, cost=117.55515, r2=0.62402\n",
            "lr=0.001, lam=10, cost=119.53693, r2=0.62396\n",
            "lr=0.001, lam=20, cost=121.70935, r2=0.6238\n",
            "lr=0.01, lam=1e-15, cost=2.39124, r2=0.99234\n",
            "lr=0.01, lam=1e-10, cost=2.39124, r2=0.99234\n",
            "lr=0.01, lam=1e-05, cost=2.39125, r2=0.99234\n",
            "lr=0.01, lam=0.001, cost=2.39147, r2=0.99234\n",
            "lr=0.01, lam=0, cost=2.39124, r2=0.99234\n",
            "lr=0.01, lam=1, cost=2.61404, r2=0.99234\n",
            "lr=0.01, lam=10, cost=4.60256, r2=0.99227\n",
            "lr=0.01, lam=20, cost=6.77918, r2=0.99211\n",
            "lr=0.1, lam=1e-15, cost=2.26382, r2=0.99275\n",
            "lr=0.1, lam=1e-10, cost=2.26382, r2=0.99275\n",
            "lr=0.1, lam=1e-05, cost=2.26382, r2=0.99275\n",
            "lr=0.1, lam=0.001, cost=2.2641, r2=0.99275\n",
            "lr=0.1, lam=0, cost=2.26382, r2=0.99275\n",
            "lr=0.1, lam=1, cost=2.53378, r2=0.99267\n",
            "lr=0.1, lam=10, cost=4.59566, r2=0.99234\n",
            "lr=0.1, lam=20, cost=6.77785, r2=0.99214\n",
            "lr=1, lam=1e-15 -> diverged\n",
            "lr=1, lam=1e-10 -> diverged\n",
            "lr=1, lam=1e-05 -> diverged\n",
            "lr=1, lam=0.001 -> diverged\n",
            "lr=1, lam=0 -> diverged\n",
            "lr=1, lam=1 -> diverged\n",
            "lr=1, lam=10 -> diverged\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3414392381.py:38: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1/(2*m))*np.sum(err**2) + (lam/(2*m))*np.sum(w[1:]**2)\n",
            "/tmp/ipython-input-3414392381.py:38: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  cost = (1/(2*m))*np.sum(err**2) + (lam/(2*m))*np.sum(w[1:]**2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr=1, lam=20 -> diverged\n",
            "lr=10, lam=1e-15 -> diverged\n",
            "lr=10, lam=1e-10 -> diverged\n",
            "lr=10, lam=1e-05 -> diverged\n",
            "lr=10, lam=0.001 -> diverged\n",
            "lr=10, lam=0 -> diverged\n",
            "lr=10, lam=1 -> diverged\n",
            "lr=10, lam=10 -> diverged\n",
            "lr=10, lam=20 -> diverged\n",
            "\n",
            "==========================\n",
            "Best Learning Rate : 0.1\n",
            "Best Lambda (Reg)  : 0\n",
            "Best R2 Score      : 0.99275\n",
            "Min Cost Function  : 2.26382\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 200\n",
        "\n",
        "a1 = np.random.rand(n)*10\n",
        "a2 = a1*0.9 + np.random.randn(n)*0.1\n",
        "a3 = a1*1.1 + np.random.randn(n)*0.2\n",
        "a4 = a2*0.8 + np.random.randn(n)*0.2\n",
        "a5 = a3*1.2 + np.random.randn(n)*0.3\n",
        "a6 = (a4+a5)/2 + np.random.randn(n)*0.2\n",
        "a7 = (a1+a3+a5)/3 + np.random.randn(n)*0.1\n",
        "\n",
        "\n",
        "y = 3*a1 + 2*a2 - a3 + 0.5*a4 + 4*a5 - 2*a6 + a7 + np.random.randn(n)*2\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'a1':a1, 'a2':a2, 'a3':a3, 'a4':a4, 'a5':a5, 'a6':a6, 'a7':a7, 'y':y\n",
        "})\n",
        "\n",
        "X = df[['a1','a2','a3','a4','a5','a6','a7']].values.astype(np.float64)\n",
        "Y = df['y'].values.reshape(-1,1).astype(np.float64)\n",
        "\n",
        "\n",
        "X = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
        "X = np.c_[np.ones((X.shape[0],1)), X]\n",
        "\n",
        "\n",
        "\n",
        "def ridge_cost(X,Y,w,lam):\n",
        "    m = len(Y)\n",
        "    preds = X.dot(w)\n",
        "    err = preds - Y\n",
        "    cost = (1/(2*m))*np.sum(err**2) + (lam/(2*m))*np.sum(w[1:]**2)\n",
        "    return cost\n",
        "\n",
        "def ridge_gradient_descent(X,Y,lr,lam,iterations=1000):\n",
        "    m,n = X.shape\n",
        "    w = np.zeros((n,1))\n",
        "    last_cost = None\n",
        "\n",
        "    for i in range(iterations):\n",
        "        preds = X.dot(w)\n",
        "        err = preds - Y\n",
        "        grad = (1/m)*(X.T.dot(err)) + (lam/m)*np.r_[[[0]],w[1:]]\n",
        "        w = w - lr*grad\n",
        "\n",
        "\n",
        "        if np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
        "\n",
        "            return None, np.inf\n",
        "        last_cost = ridge_cost(X,Y,w,lam)\n",
        "    return w, last_cost\n",
        "\n",
        "\n",
        "learning_rates = [0.0001,0.001,0.01,0.1,1,10]\n",
        "lambdas = [1e-15,1e-10,1e-5,1e-3,0,1,10,20]\n",
        "\n",
        "best_r2 = -999\n",
        "best_cost = 999999\n",
        "best_lr = None\n",
        "best_lam = None\n",
        "best_w = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        weights, final_cost = ridge_gradient_descent(X,Y,lr,lam,iterations=1000)\n",
        "\n",
        "\n",
        "        if weights is None or np.isinf(final_cost):\n",
        "            print(f\"lr={lr}, lam={lam} -> diverged\")\n",
        "            continue\n",
        "\n",
        "        preds = X.dot(weights)\n",
        "        r2 = r2_score(Y, preds)\n",
        "\n",
        "        print(f\"lr={lr}, lam={lam}, cost={round(final_cost,5)}, r2={round(r2,5)}\")\n",
        "\n",
        "        if (r2 > best_r2) or (r2 == best_r2 and final_cost < best_cost):\n",
        "            best_r2 = r2\n",
        "            best_cost = final_cost\n",
        "            best_lr = lr\n",
        "            best_lam = lam\n",
        "            best_w = weights\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"Best Learning Rate :\", best_lr)\n",
        "print(\"Best Lambda (Reg)  :\", best_lam)\n",
        "print(\"Best R2 Score      :\", round(best_r2,5))\n",
        "print(\"Min Cost Function  :\", round(best_cost,5))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2)\n",
        "Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
        "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "(b) Separate input and output features and perform scaling\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
        "regularization parameter as 0.5748) regression function on the dataset.\n",
        "(d) Evaluate the performance of each trained model on test set. Which model\n",
        "performs the best and Why?"
      ],
      "metadata": {
        "id": "DEhEBribAfNW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGSTp_l4AqhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3)\n",
        "Cross Validation for Ridge and Lasso Regression\n",
        "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
        "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
        "dataset from sklearn.datasets)."
      ],
      "metadata": {
        "id": "Uy0pl7wGBDw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print(\"Loading California Housing dataset\")\n",
        "data = fetch_california_housing()\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Columns:\", list(X.columns))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "print(\"Train/Test split completed.\")\n",
        "\n",
        "alphas_ridge = [0.001, 0.01, 0.1, 1, 5, 10, 50, 100]\n",
        "ridge_model = RidgeCV(alphas=alphas_ridge, store_cv_values=True)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "ridge_preds = ridge_model.predict(X_test)\n",
        "ridge_r2 = r2_score(y_test, ridge_preds)\n",
        "\n",
        "print(\"Ridge Regression Results\")\n",
        "print(\"Best Alpha (λ):\", ridge_model.alpha_)\n",
        "print(\"R2 Score:\", round(ridge_r2, 4))\n",
        "\n",
        "alphas_lasso = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lasso_model = LassoCV(alphas=alphas_lasso, cv=5, max_iter=10000, random_state=42)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "lasso_preds = lasso_model.predict(X_test)\n",
        "lasso_r2 = r2_score(y_test, lasso_preds)\n",
        "\n",
        "print(\"Lasso Regression Results\")\n",
        "print(\"Best Alpha (λ):\", lasso_model.alpha_)\n",
        "print(\"R2 Score:\", round(lasso_r2, 4))\n",
        "\n",
        "print(\" Model Comparison\")\n",
        "if ridge_r2 > lasso_r2:\n",
        "    print(f\"Ridge performed better with R2 = {round(ridge_r2,4)}\")\n",
        "else:\n",
        "    print(f\"Lasso performed better with R2 = {round(lasso_r2,4)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJbrzWDhBlsE",
        "outputId": "b1a4c009-4dd0-46f7-c3aa-7bf9150bd2cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading California Housing dataset\n",
            "Dataset shape: (20640, 8)\n",
            "Columns: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "Train/Test split completed.\n",
            "Ridge Regression Results\n",
            "Best Alpha (λ): 10.0\n",
            "R2 Score: 0.5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Regression Results\n",
            "Best Alpha (λ): 0.0001\n",
            "R2 Score: 0.5912\n",
            " Model Comparison\n",
            "Ridge performed better with R2 = 0.5916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4)\n",
        "Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step)\n",
        "on Iris dataset using one vs. rest strategy?"
      ],
      "metadata": {
        "id": "OlTYXRq2CpT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "labels = iris.target_names\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Unique classes:\", labels)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Train/Test split done!\")\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=labels)\n",
        "\n",
        "print(\"Model Evaluation\")\n",
        "print(\"Accuracy:\", round(acc * 100, 2), \"%\")\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "\n",
        "print(\"\\nModel coefficients (for each class):\")\n",
        "for i, cls in enumerate(labels):\n",
        "    print(f\"{cls} → Coefficients:\", model.coef_[i])\n",
        "\n",
        "sample = X_test[0].reshape(1, -1)\n",
        "predicted_class = model.predict(sample)[0]\n",
        "print(\"\\nExample Prediction:\")\n",
        "print(\"Actual Class:\", labels[y_test[0]])\n",
        "print(\"Predicted Class:\", labels[predicted_class])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyjKu_kbDAUZ",
        "outputId": "09c08732-4b8e-4a8c-cdd8-f87a7a3a8c64"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (150, 4)\n",
            "Unique classes: ['setosa' 'versicolor' 'virginica']\n",
            "Train/Test split done!\n",
            "Model Evaluation\n",
            "Accuracy: 97.37 %\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  0  0]\n",
            " [ 0 10  1]\n",
            " [ 0  0 12]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       1.00      0.91      0.95        11\n",
            "   virginica       0.92      1.00      0.96        12\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.97      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Model coefficients (for each class):\n",
            "setosa → Coefficients: [-0.4150181   0.86740485 -2.18505099 -0.90552924]\n",
            "versicolor → Coefficients: [-0.15342167 -2.09570192  0.54584218 -0.97607025]\n",
            "virginica → Coefficients: [-0.37075602 -0.50440372  2.72707974  2.02082242]\n",
            "\n",
            "Example Prediction:\n",
            "Actual Class: versicolor\n",
            "Predicted Class: versicolor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}